{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "from skimage import io, transform\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a VGG pretrained model and only select the first 23 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "newmodel = torch.nn.Sequential(*(list(vgg16.features[:24])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the dataloaders and their augmentations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation((-20,20)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], #default\n",
    "    std  = [ 0.229, 0.224, 0.225 ]) # default\n",
    "    #transforms.Normalize(mean = [ 0.71, 0.67, 0.59 ],\n",
    "    #std  = [ 0.064, 0.059, 0.05]),\n",
    "    \n",
    "    ])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], #default\n",
    "    std  = [ 0.229, 0.224, 0.225 ]) # default\n",
    "    #transforms.Normalize(mean = [ 0.71, 0.67, 0.59 ],\n",
    "    #std  = [ 0.064, 0.059, 0.05]),\n",
    "    \n",
    "    ])\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], #default\n",
    "#     std  = [ 0.229, 0.224, 0.225 ]), # default\n",
    "#     #transforms.Normalize(mean = [ 0.71, 0.67, 0.59 ],\n",
    "#     #std  = [ 0.064, 0.059, 0.05]),\n",
    "    \n",
    "#     ])\n",
    "\n",
    "trainData = datasets.ImageFolder('../data/processed/classifier_training', transform)\n",
    "trainLoader = torch.utils.data.DataLoader(dataset=trainData, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract vector encodings for drawings and photos from the VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_encodings = []\n",
    "photo_encodings = []\n",
    "for images, labels in trainLoader:\n",
    "    images = Variable(images)\n",
    "    labels = Variable(labels)\n",
    "    encoding = newmodel(images)\n",
    "    encoding = encoding.detach().numpy().flatten()\n",
    "    if labels == 0:\n",
    "        drawing_encodings.append(encoding)\n",
    "    else:\n",
    "        photo_encodings.append(encoding)\n",
    "        \n",
    "photos = np.asarray(photo_encodings)\n",
    "drawings = np.asarray(drawing_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use these encodings to train a Linear SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC(random_state=0)\n",
    "X = np.concatenate((drawings, photos),axis=0)\n",
    "y = np.concatenate((np.zeros(len(drawing_encodings)), np.ones(len(photo_encodings))),axis=0)\n",
    "\n",
    "# TODO SAVE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our accuracy is estimated at: [0.96078431 0.9        0.92       0.84       0.94       0.84\n",
      " 0.92       0.92       0.87755102 0.93877551]\n"
     ]
    }
   ],
   "source": [
    "outscore = cross_val_score(clf,X,y, cv=10)\n",
    "print('our accuracy is estimated at: ' + str(outscore)) #layer 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean(outscore), 2)\n",
    "np.round(np.std(outscore), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/processed/finalized_photo_illustration_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting type of image\n",
    "Here we predict which type of image (photo or drawing) we are dealing with. <br>\n",
    "The images are then placed in folders belonging to this type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictData = ImageFolderWithPaths('../data/processed/gender_/', transform2)\n",
    "predictLoader = torch.utils.data.DataLoader(dataset=predictData, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dir exists\n",
    "for dir_ in ['test', 'train', 'validation']:\n",
    "    os.mkdir('../data/processed/gender_/' + dir_ + '/f_photo')\n",
    "    os.mkdir('../data/processed/gender_/' + dir_ + '/m_photo')\n",
    "    os.mkdir('../data/processed/gender_/' + dir_ + '/f_drawing')\n",
    "    os.mkdir('../data/processed/gender_/' + dir_ + '/m_drawing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels, paths in predictLoader:\n",
    "    path = ' '.join(paths)\n",
    "    path_ = path.split('/')\n",
    "    images = Variable(images)\n",
    "    encoding = newmodel(images)\n",
    "    encoding = encoding.detach().numpy().flatten()\n",
    "    y_predict = clf.predict((np.asarray(encoding).reshape(1,-1)))\n",
    "    if y_predict == 0:\n",
    "        newpath ='../data/processed/gender_/' + path_[4] + '/' + path_[5] + '_drawing/' + path_[6]\n",
    "        shutil.copy(path, newpath)\n",
    "    else:\n",
    "        newpath = '../data/processed/gender_/' + path_[4] + '/' + path_[5] + '_photo/' + path_[6]\n",
    "        shutil.copy(path, newpath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/processed/KB_faces/1978/KBNRC01-000026215-mpeg21-a0080.jpg'\n",
    "\n",
    "preprocess_img = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "newmodel = torch.nn.Sequential(*(list(vgg16.features[:24])))\n",
    "\n",
    "filename = \"finalized_photo_illustration_model.sav.sav\"\n",
    "clf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(img_path)\n",
    "img_tensor = preprocess_img(img)\n",
    "img_tensor.unsqueeze_(0)\n",
    "images = Variable(img_tensor)\n",
    "encoding = newmodel(images)\n",
    "encoding = encoding.detach().numpy().flatten()\n",
    "prediction = clf.predict((np.asarray(encoding).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
